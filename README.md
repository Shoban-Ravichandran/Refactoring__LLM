# A Transformer Based Framework for Automatically refactoring Legacy Code

A comprehensive Retrieval-Augmented Generation (RAG) system for Python legacy code refactoring suggestions, featuring multi-objective optimization for model selection and enhanced evaluation metrics.

## Features

- **Multi-LLM Support**: Integration with multiple language models (Groq)
- **Advanced Retrieval**: Code-specific embeddings with enhanced query processing
- **Multi-Objective Optimization**: NSGA-II algorithm for optimal model selection
- **Comprehensive Evaluation**: RAG metrics including BLEU, ROUGE, context relevance, and faithfulness
- **Code Quality Analysis**: Cyclomatic complexity, maintainability index, and code smell detection
- **PDF Processing**: Extract and process refactoring knowledge from PDF documents
- **Vector Storage**: Qdrant integration for efficient similarity search
- **Interactive Demo**: Easy-to-use demo mode for demonstrations
- **Legacy Code Generation**: Automatic generation of 2200+ refactoring examples

## Project Structure

```
python-refactoring-rag/
├── README.md
├── requirements.txt
├── main.py                          # Main application entry point
├── demo.py                          # Interactive demo mode
├── key.env                          # Environment variables (create from .env.example)
├── .env.example                     # Environment variables template
├── 
├── config/                          # Configuration management
│   ├── __init__.py
│   ├── settings.py                  # Core settings and constants
│   └── model_configs.py             # LLM model configurations
│
├── data/                            # Data processing and generation
│   ├── __init__.py
│   ├── generators/
│   │   ├── __init__.py
│   │   └── legacy_code_generator.py # Dataset generation (generates JSON in inputs/)
│   ├── processors/
│   │   ├── __init__.py
│   │   ├── code_chunker.py          # Code chunking logic
│   │   └── pdf_processor.py         # PDF text extraction
│   └── embeddings/
│       ├── __init__.py
│       └── code_embedder.py         # Code-specific embeddings
│
├── models/                          # Core algorithmic components
│   ├── __init__.py
│   ├── llm_providers.py             # Multi-LLM provider management
│   ├── evaluation/
│   │   ├── __init__.py
│   │   └── rag_evaluator.py         # RAG evaluation metrics
│   └── optimization/
│       ├── __init__.py
│       └── nsga2_optimizer.py       # Multi-objective optimization
│
├── services/                        # Business logic and orchestration
│   ├── __init__.py
│   ├── vector_store.py              # Qdrant vector database
│   ├── retrieval_service.py         # Enhanced retrieval logic
│   ├── query_processor.py           # Query enhancement
│   ├── rag_service.py               # Main RAG system orchestration
│   └── interactive_service.py       # Interactive demo service
│
├── utils/                           # Generic utilities
│   ├── __init__.py
│   └── logging_utils.py             # Logging configuration
│
├── inputs/                          # Data files (created automatically)
│   ├── datasets/
│   │   ├── python_legacy_refactoring_dataset.json  # Generated by legacy_code_generator.py
│   │   └── dataset_statistics.json                 # Generated statistics
│   └── expert_knowledge/                           # Place your PDF books here
│       ├── clean-code-in-python.pdf               # Your PDF books
│       └── the-clean-coder.pdf                    # Your PDF books
│
└── tests/                           # Unit tests
    ├── __init__.py
```

## Installation

### Prerequisites
- Python 3.8 or higher
- At least one LLM API key (Groq, OpenAI, or Anthropic)

### Step 1: Clone the Repository
```bash
git clone <repository-url>
cd python-refactoring-rag
```

### Step 2: Install Dependencies
```bash
pip install -r requirements.txt
```

### Step 3: Set Up Environment Variables
```bash
# Copy the example environment file
cp .env.example key.env

# Edit key.env with your API keys
nano key.env  # or use your preferred editor
```

**Required environment variables in `key.env`:**
```bash
# At least one of these is required
GROQ_API_KEY=your_groq_api_key_here
OPENAI_API_KEY=your_openai_api_key_here
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Optional: Qdrant Cloud (uses local Qdrant if not specified)
QDRANT_URL=https://your-cluster.qdrant.io
QDRANT_API_KEY=your_qdrant_api_key_here
```

### Step 4: Generate Dataset
```bash
# Generate the legacy code refactoring dataset
python -m data.generators.legacy_code_generator

# This creates inputs/datasets/python_legacy_refactoring_dataset.json
```

### Step 5: Add Expert Knowledge (Optional)
Place PDF books about clean code and refactoring in `inputs/expert_knowledge/`:
```
inputs/expert_knowledge/
├── clean-code-in-python.pdf
├── the-clean-coder.pdf
├── refactoring-improving-design.pdf
└── effective-python.pdf
```

## Quick Start

### Option 1: Interactive Demo

Perfect for demonstrations and testing:

```bash
python demo.py
```

**Demo Features:**
- Pre-configured sample queries
- Minimal setup required
- Rich console formatting
- Interactive code analysis
- No evaluation overhead

**Demo Menu Options:**
1. **Run sample queries** - Automated demo with example refactoring scenarios
2. **Interactive mode** - Ask your own questions and paste code
3. **System statistics** - View system status and capabilities

### Option 2: Full System (Complete Experience)

For the complete system with evaluation and optimization:

```bash
python main.py
```

**Full System Features:**
- Comprehensive model evaluation
- NSGA-II multi-objective optimization
- Model performance comparison
- Interactive mode with optimized best model

**Main Menu Options:**
1. **Run full evaluation and optimization** - Complete system analysis
2. **Interactive mode (quick start)** - Skip optimization, go straight to interaction
3. **System statistics** - Detailed system information
4. **Health check** - Verify all components are working


## Usage Examples

### Basic Refactoring Suggestion

```python
from services.rag_service import RefactoringRAGSystem
from config.model_configs import get_default_llm_configs

# Initialize system
system = RefactoringRAGSystem(get_default_llm_configs())
system.setup()

# Load data (if not already loaded)
system.process_dataset("inputs/datasets/python_legacy_refactoring_dataset.json")

# Get refactoring suggestions
query = "How can I reduce complexity in nested loops?"
suggestions = system.get_refactoring_suggestions(query)

if isinstance(suggestions, dict):
    for model_name, suggestion in suggestions.items():
        print(f"\n=== {model_name} ===")
        print(suggestion)
else:
    print(suggestion)
```

### Interactive Code Analysis

```python
# Analyze specific code
messy_code = '''
def process_orders(orders, discount_codes, tax_rates):
    results = []
    for order in orders:
        if order['status'] == 'pending':
            total = 0
            for item in order['items']:
                if item['quantity'] > 0:
                    item_total = item['price'] * item['quantity']
                    if order.get('discount_code') in discount_codes:
                        if discount_codes[order['discount_code']]['type'] == 'percentage':
                            discount = item_total * (discount_codes[order['discount_code']]['value'] / 100)
                        else:
                            discount = min(discount_codes[order['discount_code']]['value'], item_total)
                        item_total -= discount
                    total += item_total
            results.append({'order_id': order['id'], 'total': round(total, 2)})
    return results
'''

suggestion = system.get_refactoring_suggestions(
    "How can I simplify this complex nested function?",
    user_code=messy_code
)
print(suggestion)
```

### Custom Evaluation

```python
from models.evaluation.rag_evaluator import RAGEvaluator
from config.settings import EvaluationConfig

evaluator = RAGEvaluator()
config = EvaluationConfig()

# Define test cases
test_cases = [
    {
        'query': 'How can I make this code more readable?',
        'original_code': 'def calc(x): return x**2 if x>0 else 0',
        'reference_answer': 'Use descriptive names and proper formatting...'
    }
]

# Evaluate models
results = evaluator.evaluate_multiple_queries(test_cases)
for result in results:
    if result['success']:
        metrics = result['rag_metrics']
        print(f"Context Relevance: {metrics.context_relevance:.3f}")
        print(f"Answer Relevance: {metrics.answer_relevance:.3f}")
        print(f"Faithfulness: {metrics.faithfulness:.3f}")
```

## Configuration

### LLM Model Configuration

The system supports multiple LLM providers. Configure them in `key.env`:

```bash
# Primary provider (Groq - fast and cost-effective)
GROQ_API_KEY=your_groq_key

# Secondary providers (optional)
OPENAI_API_KEY=your_openai_key
ANTHROPIC_API_KEY=your_anthropic_key
```

**Available Models by Provider:**

**Groq Models:**
- `llama3-70b-8192` - Large, high-quality responses
- `gemma2-9b-it` - Balanced performance
- `qwen/qwen3-32b` - Alternative large model
- `moonshotai/kimi-k2-instruct` - Specialized model
- `deepseek-r1-distill-llama-70b` - Optimized model

**OpenAI Models:**
- `gpt-4` - Highest quality (expensive)
- `gpt-3.5-turbo` - Fast and cost-effective

**Anthropic Models:**
- `claude-3-opus-20240229` - Highest quality
- `claude-3-sonnet-20240229` - Balanced performance

### System Configuration

Adjust system behavior in `config/settings.py`:

```python
@dataclass
class RAGConfig:
    top_k: int = 5                    # Number of retrieved chunks
    similarity_threshold: float = 0.3  # Minimum similarity score
    max_context_length: int = 3000    # Maximum context length
    include_metrics: bool = True      # Include performance metrics
    focus_areas: List[str] = field(default_factory=lambda: [
        'complexity', 'performance', 'readability', 'patterns'
    ])

@dataclass  
class EvaluationConfig:
    context_relevance_weight: float = 0.25
    answer_relevance_weight: float = 0.20
    faithfulness_weight: float = 0.20
    completeness_weight: float = 0.15
    bleu_weight: float = 0.10
    rouge_weight: float = 0.10
```

### Vector Database Configuration

**Local Qdrant (Default):**
```python
# No additional configuration needed
# System automatically uses local Qdrant instance
```

**Qdrant Cloud:**
```bash
# Add to key.env
QDRANT_URL=https://your-cluster.qdrant.io
QDRANT_API_KEY=your_qdrant_key
```

## Advanced Features

### Multi-Objective Optimization with NSGA-II

The system uses NSGA-II (Non-dominated Sorting Genetic Algorithm II) to find optimal model configurations:

```python
from models.optimization.nsga2_optimizer import run_nsga2_optimization

# After evaluation, run optimization
optimization_results = run_nsga2_optimization(evaluation_results)

print(f"Best Model: {optimization_results['best_model']}")
print(f"Pareto Front Size: {optimization_results['pareto_front_size']}")
print(f"Optimization Time: {optimization_results['optimization_time_seconds']:.2f}s")

# Access all Pareto optimal solutions
for solution in optimization_results['all_pareto_solutions']:
    print(f"Model: {solution['model']}, Rank: {solution['rank']}")
```

**Optimization Objectives:**
1. **Context Relevance** - How well retrieved chunks match the query
2. **Answer Relevance** - How well the answer addresses the question
3. **Faithfulness** - How well the answer is grounded in the context
4. **Response Completeness** - How comprehensive the answer is
5. **BLEU Score** - N-gram overlap with reference answers
6. **ROUGE Score** - Recall-oriented overlap with reference answers

### PDF Knowledge Integration

Automatically extract and index knowledge from PDF books:

```python
# PDFs are automatically processed when found in expert_knowledge/
pdf_files = [
    "inputs/expert_knowledge/clean-code-in-python.pdf",
    "inputs/expert_knowledge/the-clean-coder.pdf"
]

# Process PDFs
chunks_processed = system.process_pdfs(pdf_files)
print(f"Processed {chunks_processed} chunks from PDFs")

# Check PDF processing status
pdf_summary = system.get_pdf_processing_summary()
print(f"Total PDF chunks: {pdf_summary['total_chunks']}")
for filename, stats in pdf_summary['pdf_files'].items():
    print(f"  {filename}: {stats['total_chunks']} chunks")
```

**Supported PDF Features:**
- Text extraction using PyMuPDF (preferred) or PyPDF2 (fallback)
- Code block detection and extraction
- Intelligent chunking with overlap
- Metadata preservation
- Duplicate detection and skipping

### Comprehensive Evaluation Metrics

The system provides detailed evaluation metrics:

```python
from models.evaluation.rag_evaluator import RAGEvaluator

evaluator = RAGEvaluator()

# Single query evaluation
result = evaluator.evaluate_single_query(
    query="How can I improve this code?",
    context_chunks=retrieved_chunks,
    answer=model_response,
    reference_answer=expected_answer
)

if result['success']:
    metrics = result['rag_metrics']
    print(f"Context Relevance: {metrics.context_relevance:.3f}")
    print(f"Answer Relevance: {metrics.answer_relevance:.3f}")
    print(f"Faithfulness: {metrics.faithfulness:.3f}")
    print(f"Response Completeness: {metrics.response_completeness:.3f}")
    print(f"BLEU Score: {metrics.bleu_score:.4f}")
    print(f"ROUGE-L Score: {metrics.rouge_l_score:.4f}")
    print(f"Processing Time: {metrics.latency_ms:.1f}ms")
```

### Custom Refactoring Patterns

Add new refactoring patterns to the system:

1. **Add pattern definition** in `config/settings.py`:
```python
REFACTORING_PATTERNS = {
    'introduce_design_pattern': {
        'name': 'Introduce Design Pattern',
        'description': 'Apply appropriate design patterns',
        'category': 'design_patterns',
        'complexity_reduction': 0.4,
        'readability_improvement': 0.7
    }
}
```

2. **Implement pattern generator** in `data/generators/legacy_code_generator.py`:
```python
def _generate_design_pattern_example(self) -> Tuple[str, str, str]:
    original = '''
    # Original code without design pattern
    def process_data(data, processor_type):
        if processor_type == 'json':
            # JSON processing logic
        elif processor_type == 'xml':
            # XML processing logic
        # ...
    '''
    
    refactored = '''
    # Refactored code with Strategy pattern
    class DataProcessor(ABC):
        @abstractmethod
        def process(self, data): pass
    
    class JSONProcessor(DataProcessor):
        def process(self, data): # JSON logic
    
    class XMLProcessor(DataProcessor):
        def process(self, data): # XML logic
    '''
    
    description = "Apply Strategy pattern to eliminate conditional logic"
    return original, refactored, description
```

## ataset Generation

The system includes a comprehensive legacy code generator that creates realistic refactoring examples:

### Generate Dataset

```bash
# Generate 2200+ examples across 10 refactoring patterns
python -m data.generators.legacy_code_generator
```

### Generated Patterns

The generator creates examples for these refactoring patterns:

1. **Extract Method** - Breaking down long functions
2. **Extract Class** - Creating classes from related functionality
3. **Replace Conditional with Polymorphism** - Using inheritance instead of conditionals
4. **Introduce Parameter Object** - Grouping related parameters
5. **Replace Loop with Comprehension** - Using Python list/dict comprehensions
6. **Add Type Hints** - Adding type annotations
7. **Improve Error Handling** - Adding proper exception handling
8. **Eliminate Code Duplication** - Removing repeated code
9. **Improve Naming** - Using descriptive names
10. **Optimize Performance** - Improving algorithm efficiency

### Dataset Structure

Each generated example includes:

```json
{
  "id": "legacy_example_0001",
  "original_code": "def process_data(data): ...",
  "refactored_code": "def process_user_data(user_data_list): ...",
  "description": "Break down the monolithic function...",
  "refactoring_type": "extract_method",
  "complexity_before": 15,
  "complexity_after": 6,
  "benefits": ["reduced_complexity", "improved_readability"],
  "tags": ["functions", "complexity", "organization"],
  "context": {
    "domain": "web_development",
    "function_purpose": "data_processing",
    "legacy_indicators": ["long_function", "nested_loops"]
  },
  "code_smells_detected": ["long_function", "data_clumps"],
  "maintainability_score_before": 4.2,
  "maintainability_score_after": 8.1
}
```

## Development

### Code Quality

```bash
# Install development tools
pip install black isort flake8 mypy

# Format code
black .
isort .

# Check style
flake8 .

# Type checking
mypy .
```

### Adding New Components

#### New LLM Provider

1. **Add provider configuration** in `config/model_configs.py`:
```python
def get_custom_models() -> List[LLMConfig]:
    api_key = os.getenv('CUSTOM_API_KEY')
    return [
        LLMConfig(
            model_name="custom-model-name",
            provider=LLMProvider.CUSTOM,
            api_key=api_key
        )
    ]
```

2. **Implement provider logic** in `models/llm_providers.py`:
```python
def _initialize_custom_client(self, config: LLMConfig):
    # Initialize custom client
    pass

def _generate_custom_suggestion(self, query, context, config):
    # Generate suggestion using custom provider
    pass
```

#### New Evaluation Metric

1. **Add metric to evaluator** in `models/evaluation/rag_evaluator.py`:
```python
def evaluate_custom_metric(self, query: str, answer: str) -> float:
    # Implement custom evaluation logic
    return score

def comprehensive_evaluation(self, ...):
    # Add custom metric to existing evaluation
    custom_score = self.evaluate_custom_metric(query, answer)
    # Include in RAGEvaluationMetrics
```

2. **Update metrics dataclass**:
```python
@dataclass
class RAGEvaluationMetrics:
    # ... existing metrics
    custom_metric: float = 0.0
```

## Troubleshooting

### Common Issues

#### 1. No API Keys Found
```bash
Error: No valid LLM configurations found. Please check your API keys.
```

**Solution:**
- Ensure `key.env` file exists in the project root
- Verify at least one API key is set correctly
- Check that API keys are valid and have sufficient credits

#### 2. Dataset Not Found
```bash
Warning: Dataset file not found
```

**Solution:**
```bash
# Generate the dataset
python -m data.generators.legacy_code_generator

# Verify file was created
ls inputs/datasets/python_legacy_refactoring_dataset.json
```

#### 3. Vector Store Connection Failed
```bash
Error: Vector store connection failed
```

**Solution for Local Qdrant:**
```bash
# Install and run local Qdrant
docker run -p 6333:6333 qdrant/qdrant
```

**Solution for Qdrant Cloud:**
- Verify `QDRANT_URL` and `QDRANT_API_KEY` in `key.env`
- Check network connectivity to Qdrant Cloud

#### 4. PDF Processing Fails
```bash
Error: Failed to process PDF
```

**Solution:**
```bash
# Install PDF processing dependencies
pip install PyMuPDF PyPDF2

# Check PDF file permissions and format
# Ensure PDFs are text-based, not scanned images
```

#### 5. Evaluation Errors
```bash
Error: Model evaluation failed
```

**Solution:**
- Check API rate limits
- Verify model availability
- Ensure sufficient API credits
- Try with fewer test cases

#### 6. Memory Issues with Large Datasets
```bash
MemoryError: Unable to allocate memory
```

**Solution:**
```python
# Process in smaller batches
system.process_dataset(dataset_path, batch_size=100)

# Use cloud vector store instead of local
# Reduce chunk size in configuration
```

### Performance Optimization

#### 1. Speed Up Processing
```python
# Use cloud Qdrant for better performance
system = RefactoringRAGSystem(
    llm_configs=configs,
    qdrant_url="https://your-cluster.qdrant.io",
    qdrant_api_key="your-key"
)

# Reduce embedding batch size if memory is limited
system.embedder.batch_size = 16
```

#### 2. Optimize Retrieval
```python
# Adjust retrieval parameters
config = RAGConfig(
    top_k=3,                    # Reduce for faster retrieval
    similarity_threshold=0.4,   # Higher threshold for better quality
    max_context_length=2000     # Reduce for faster processing
)
```

#### 3. Efficient Evaluation
```python
# Use subset of models for faster evaluation
selected_models = ["llama3-70b-8192", "gemma2-9b-it"]
evaluation_results = {}
for model in selected_models:
    # Run evaluation only on selected models
```

### Debug Mode

Enable detailed logging for troubleshooting:

```python
import logging
logging.basicConfig(level=logging.DEBUG)

# Or set in environment
LOG_LEVEL=DEBUG python main.py
```

## API Reference

### Core Classes

#### RefactoringRAGSystem
Main system orchestrator for the entire RAG pipeline.

```python
class RefactoringRAGSystem:
    def __init__(self, llm_configs: List[LLMConfig], 
                 qdrant_url: str = None, qdrant_api_key: str = None)
    
    def setup(self, embedding_model: str = None, config: Dict = None)
    def process_dataset(self, dataset_path: str, force_reindex: bool = False) -> int
    def process_pdfs(self, pdf_paths: List[str], force_reindex: bool = False) -> int
    def get_refactoring_suggestions(self, query: str, **kwargs) -> Union[str, Dict[str, str]]
    def health_check(self) -> Dict[str, bool]
    def get_system_stats(self) -> Dict[str, Any]
```

#### RAGEvaluator
Comprehensive evaluation of RAG system performance.

```python
class RAGEvaluator:
    def __init__(self, config: EvaluationConfig = None)
    
    def evaluate_single_query(self, query: str, context_chunks: List[Dict], 
                              answer: str, **kwargs) -> Dict[str, Any]
    def evaluate_multiple_queries(self, test_cases: List[Dict]) -> List[Dict]
    def calculate_aggregate_metrics(self, results: List[Dict]) -> Dict[str, float]
```

#### InteractiveService
User interaction and demonstration interface.

```python
class InteractiveService:
    def __init__(self, rag_system)
    
    def run(self)  # Start interactive session
```

### Key Methods

#### Data Processing
```python
# Generate dataset
from data.generators.legacy_code_generator import main as generate_dataset
generate_dataset()

# Process existing dataset
system.process_dataset("path/to/dataset.json")

# Process PDF files
system.process_pdfs(["path/to/book1.pdf", "path/to/book2.pdf"])
```

#### Querying
```python
# Single model suggestion
suggestion = system.get_refactoring_suggestions(
    query="How can I improve this code?",
    model_name="llama3-70b-8192",
    user_code="def messy_function(): ..."
)

# All models
suggestions = system.get_refactoring_suggestions(
    query="How can I improve this code?",
    user_code="def messy_function(): ..."
)

# Best model only (after optimization)
suggestion = system.get_best_model_suggestion(
    query="How can I improve this code?",
    user_code="def messy_function(): ..."
)
```

#### Evaluation
```python
# Evaluate single query
result = evaluator.evaluate_single_query(
    query="How to refactor this code?",
    context_chunks=retrieved_chunks,
    answer=model_response,
    reference_answer="Expected answer..."
)

# Batch evaluation
results = evaluator.evaluate_multiple_queries(test_cases)

# Calculate aggregates
aggregates = evaluator.calculate_aggregate_metrics(results)
```

#### Optimization
```python
# Run NSGA-II optimization
from models.optimization.nsga2_optimizer import run_nsga2_optimization

optimization_results = run_nsga2_optimization(evaluation_results)
best_model = optimization_results['best_model']
system.set_best_model(best_model, optimization_results)
```

### Configuration Classes

```python
@dataclass
class RAGConfig:
    top_k: int = 5
    similarity_threshold: float = 0.3
    max_context_length: int = 3000
    include_metrics: bool = True
    focus_areas: List[str] = field(default_factory=list)

@dataclass
class EvaluationConfig:
    include_bleu: bool = True
    include_rouge: bool = True
    context_relevance_weight: float = 0.25
    answer_relevance_weight: float = 0.20
    faithfulness_weight: float = 0.20
    completeness_weight: float = 0.15
    bleu_weight: float = 0.10
    rouge_weight: float = 0.10

@dataclass
class OptimizationConfig:
    population_size: int = 150
    n_generations: int = 250
    crossover_prob: float = 0.9
    mutation_prob: float = 0.2
```

## Contributing

We welcome contributions! Here's how to get started:

### Development Setup

1. **Fork and clone the repository**:
```bash
git clone https://github.com/your-username/python-refactoring-rag.git
cd python-refactoring-rag
```

2. **Create a development environment**:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

3. **Install development dependencies**:
```bash
pip install pytest black isort flake8 mypy pre-commit
```

4. **Set up pre-commit hooks** (optional):
```bash
pre-commit install
```

### Development Workflow

1. **Create a feature branch**:
```bash
git checkout -b feature/your-feature-name
```

2. **Make your changes**:
   - Follow PEP 8 style guidelines
   - Add docstrings to new functions/classes
   - Include type hints
   - Write tests for new functionality

3. **Test your changes**:
```bash
# Run tests
python -m pytest tests/

# Check code style
black . --check
isort . --check-only
flake8 .

# Type checking
mypy .
```

4. **Format code** (if needed):
```bash
black .
isort .
```

5. **Commit and push**:
```bash
git add .
git commit -m "Add feature: description of your changes"
git push origin feature/your-feature-name
```

6. **Create a pull request** on GitHub

### Contribution Guidelines

#### Code Style
- Follow PEP 8
- Use descriptive variable and function names
- Add type hints to function signatures
- Include comprehensive docstrings


#### Testing
- Write unit tests for new functionality
- Ensure existing tests pass
- Aim for good test coverage
- Test with different model configurations

#### Documentation
- Update README.md if adding new features
- Add docstrings to all public functions/classes
- Include usage examples for new features
- Update API reference if needed

## License

MIT License

Copyright (c) 2024 Python Refactoring RAG System

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

## Acknowledgments

### Open Source Libraries
- **Qdrant**: Vector similarity search engine
- **Sentence Transformers**: Semantic embeddings
- **PyMuPDF**: PDF text extraction
- **Rich**: Enhanced console output
- **NLTK**: Natural language processing
- **ROUGE**: Evaluation metrics
- **pymoo**: Multi-objective optimization

### Research
- Papers on retrieval-augmented generation
- Studies on code quality metrics
- Research on automated refactoring tools
- Work on large language models for code

---
Happy refactoring!  